{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QFVQFGq5fhy"
      },
      "source": [
        "# ðŸš€ Neural Network-Based Text Compression\n",
        "\n",
        "#### ðŸ–‹ï¸ Authors\n",
        "\n",
        "- Feidnand Eide\n",
        "- Seran Shanmugathas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scNmIktH5uGe"
      },
      "source": [
        "## ðŸ“š Install Libraries\n",
        "\n",
        "We will need the following libraries:\n",
        "\n",
        "- `pytorch`\n",
        "- `pytorch-lightning`\n",
        "- `pandas`\n",
        "- `numpy`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCUvtIEO50zN"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas torch torch_geometric pytorch-lightning --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOOW_DnB6pJM"
      },
      "source": [
        "## ðŸ“Œ Import Dependencies\n",
        "\n",
        "The following libraries are used in this project:\n",
        "\n",
        "- Standard libraries: `enum`\n",
        "- PyTorch and PyTorch Lightning for model building and training\n",
        "- Pandas for data handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_ypwDRnyDv3"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "from enum import Enum\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# PyTorch geometric\n",
        "import torch_geometric\n",
        "import torch_geometric.data as geom_data\n",
        "import torch_geometric.nn as geom_nn\n",
        "from torch_geometric.nn import GraphConv\n",
        "\n",
        "# PL callbacks\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H6PJz0d7ZFB"
      },
      "source": [
        "## ðŸ”§ Configuration\n",
        "\n",
        "Set up the configuration for the model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhtWBGqd7Zwx"
      },
      "outputs": [],
      "source": [
        "config: dict = {\n",
        "    \"data_path\": \"dataset_tsmc2014/dataset_TSMC2014_NYC.txt\",\n",
        "    \"save_path\": \"models/model.pth\",\n",
        "    \"batch_size\": 32,\n",
        "    \"max_length\": 512,\n",
        "    \"input_channels\": 2,\n",
        "    \"output_channels\": 2,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout_rate\": 0.5,\n",
        "    \"max_epochs\": 1,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"num_workers\": 11,\n",
        "    \"log_every_n_steps\": 20,\n",
        "    \"pin_memory\": True if torch.cuda.is_available() else False,\n",
        "    \"accelerator\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOCTMcnO7iWu"
      },
      "source": [
        "## ðŸ—‚ï¸ Load and Preprocess the Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl8pG26T-SjH"
      },
      "source": [
        "> Enum for defining columns in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pvImQujyOp1"
      },
      "outputs": [],
      "source": [
        "class Columns(Enum):\n",
        "    \"\"\"\n",
        "    Enum containing the columns of the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    USER_ID = \"User ID\"\n",
        "    VENUE_ID = \"Venue ID\"\n",
        "    VENUE_CATEGORY_ID = \"Venue category ID\"\n",
        "    VENUE_CATEGORY_NAME = \"Venue category name\"\n",
        "    LATITUDE = \"Latitude\"\n",
        "    LONGITUDE = \"Longitude\"\n",
        "    TIMEZONE = \"Timezone\"\n",
        "    UTC_TIME = \"UTC time\"\n",
        "\n",
        "    # The following columns are not present in the dataset, but are added\n",
        "    # during the preprocessing phase.\n",
        "    HOUR = \"Hour\"\n",
        "    WEEKDAY = \"Weekday\"\n",
        "    IS_WEEKEND = \"Is weekend\"\n",
        "    NEXT_VENUE_ID = \"Next venue ID\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Mt9xtuySj7"
      },
      "outputs": [],
      "source": [
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the data as a dataframe\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "        The path to load the data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The dataset\n",
        "    \"\"\"\n",
        "    columns = [column.value for column in Columns]\n",
        "    return pd.read_csv(path, sep=\"\\t\", encoding=\"latin-1\", names=columns)\n",
        "\n",
        "\n",
        "df = load_data(config[\"data_path\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "f0miqyLx9fCp",
        "outputId": "835b4247-ee0a-414d-8121-91d67112bfb0"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Create dataset class and data module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TMSCDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, transform=None):\n",
        "        \"\"\"\n",
        "        Dataset class for the TSMC dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: pd.DataFrame\n",
        "            The dataset\n",
        "        transform: Callable\n",
        "            The transform to apply to the dataset\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: int\n",
        "            The index of the item\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            The item\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        sample = self.data.iloc[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TMSCDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size: int, data_path: str):\n",
        "        \"\"\"\n",
        "        Data module for the TSMC dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            The batch size\n",
        "        data_path: str\n",
        "            The path to the dataset\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_path = data_path\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        # Download the dataset if it doesn't exist already\n",
        "        if not os.path.exists(self.data_path):\n",
        "            os.system(\n",
        "                \"wget http://www-public.tem-tsp.eu/~zhang_da/pub/dataset_tsmc2014.zip\"\n",
        "            )\n",
        "            os.system(\"unzip dataset_tsmc2014.zip -d dataset_tsmc2014\")\n",
        "            # Assuming the dataset is unzipped into a directory named 'dataset_tsmc2014'\n",
        "\n",
        "    def setup(self, stage: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Setup the dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stage: str\n",
        "            The stage of the training\n",
        "        \"\"\"\n",
        "        data = load_data(self.data_path)\n",
        "        data = self.preprocess_data(data)\n",
        "        data = self.create_graph_data(data)\n",
        "\n",
        "        train, validate, test = np.split(\n",
        "            data.sample(frac=1, random_state=42),\n",
        "            [int(0.6 * len(data)), int(0.8 * len(data))],\n",
        "        )\n",
        "\n",
        "        self.train_dataset = TMSCDataset(train)\n",
        "        self.val_dataset = TMSCDataset(validate)\n",
        "        self.test_dataset = TMSCDataset(test)\n",
        "\n",
        "    def preprocess_data(self, data: pd.DataFrame) -> Tensor:\n",
        "        \"\"\"\n",
        "        Preprocess the data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: pd.DataFrame\n",
        "            The data to preprocess\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The preprocessed data\n",
        "        \"\"\"\n",
        "        # Parse timestamps and create temporal features\n",
        "        data[Columns.UTC_TIME.value] = pd.to_datetime(\n",
        "            data[Columns.UTC_TIME.value], format=\"%a %b %d %H:%M:%S +0000 %Y\"\n",
        "        )\n",
        "        data[Columns.HOUR.value] = data[Columns.UTC_TIME.value].dt.hour\n",
        "        data[Columns.WEEKDAY.value] = data[Columns.UTC_TIME.value].dt.weekday\n",
        "        data[Columns.IS_WEEKEND.value] = (\n",
        "            data[Columns.WEEKDAY.value].isin([5, 6]).astype(int)\n",
        "        )\n",
        "\n",
        "        # Normalize geospatial features\n",
        "        scaler = preprocessing.MinMaxScaler()\n",
        "        data[[Columns.LATITUDE.value, Columns.LONGITUDE.value]] = scaler.fit_transform(\n",
        "            data[[Columns.LATITUDE.value, Columns.LONGITUDE.value]]\n",
        "        )\n",
        "\n",
        "        # Encode categorical features using LabelEncoder\n",
        "        label_encoder_user = preprocessing.LabelEncoder()\n",
        "        label_encoder_venue = preprocessing.LabelEncoder()\n",
        "        label_encoder_category = preprocessing.LabelEncoder()\n",
        "\n",
        "        data[Columns.USER_ID.value] = label_encoder_user.fit_transform(\n",
        "            data[Columns.USER_ID.value]\n",
        "        )\n",
        "        data[Columns.VENUE_ID.value] = label_encoder_venue.fit_transform(\n",
        "            data[Columns.VENUE_ID.value]\n",
        "        )\n",
        "        data[Columns.VENUE_CATEGORY_ID.value] = label_encoder_category.fit_transform(\n",
        "            data[Columns.VENUE_CATEGORY_ID.value]\n",
        "        )\n",
        "\n",
        "        return data\n",
        "\n",
        "    def create_graph_data(self, data: pd.DataFrame) -> geom_data.Data:\n",
        "        \"\"\"\n",
        "        Create the graph data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: pd.DataFrame\n",
        "            The data to create the graph data from\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        geom_data.Data\n",
        "            The graph data\n",
        "        \"\"\"\n",
        "        # Encoding categorical features to integers\n",
        "        user_encoder = preprocessing.LabelEncoder()\n",
        "        venue_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "        data[Columns.USER_ID.value] = user_encoder.fit_transform(\n",
        "            data[Columns.USER_ID.value]\n",
        "        )\n",
        "        data[Columns.VENUE_ID.value] = venue_encoder.fit_transform(\n",
        "            data[Columns.VENUE_ID.value]\n",
        "        )\n",
        "\n",
        "        # Creating the edge index\n",
        "        def create_edge_index(data: pd.DataFrame) -> Tensor:\n",
        "            \"\"\"\n",
        "            Create the edge index\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            data: pd.DataFrame\n",
        "                The data to create the edge index from\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            Tensor\n",
        "                The edge index\n",
        "            \"\"\"\n",
        "            edge_index = []\n",
        "            for user_id in data[Columns.USER_ID.value].unique():\n",
        "                user_df = data[data[Columns.USER_ID.value] == user_id]\n",
        "                user_edges = [\n",
        "                    (\n",
        "                        user_df.iloc[i][Columns.VENUE_ID.value],\n",
        "                        user_df.iloc[i + 1][Columns.VENUE_ID.value],\n",
        "                    )\n",
        "                    for i in range(len(user_df) - 1)\n",
        "                ]\n",
        "                edge_index.extend(user_edges)\n",
        "\n",
        "            return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        edge_index = create_edge_index(data)\n",
        "\n",
        "        # Creating the node features and labels\n",
        "        def create_labels_for_next_poi(data: pd.DataFrame) -> pd.DataFrame:\n",
        "            \"\"\"\n",
        "            Create the labels for the next POI\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            data: pd.DataFrame\n",
        "                The data to create the labels from\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            pd.DataFrame\n",
        "                The data with the labels\n",
        "            \"\"\"\n",
        "            data[Columns.NEXT_VENUE_ID.value] = data[Columns.VENUE_ID.value].shift(-1)\n",
        "            return data\n",
        "\n",
        "        node_features = torch.tensor(\n",
        "            data[[Columns.LATITUDE.value, Columns.LONGITUDE.value]].values,\n",
        "            dtype=torch.float,\n",
        "        )\n",
        "        data = create_labels_for_next_poi(data)\n",
        "        labels = torch.tensor(\n",
        "            data[[Columns.NEXT_VENUE_ID.value]].values, dtype=torch.long\n",
        "        )\n",
        "\n",
        "        return geom_data.Data(\n",
        "            x=node_features, edge_index=edge_index, y=labels.squeeze()\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self) -> geom_data.DataLoader:\n",
        "        \"\"\"\n",
        "        Get the training dataloader\n",
        "        \"\"\"\n",
        "        return geom_data.DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=config[\"num_workers\"],\n",
        "            pin_memory=config[\"pin_memory\"],\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self) -> geom_data.DataLoader:\n",
        "        \"\"\"\n",
        "        Get the validation dataloader\n",
        "        \"\"\"\n",
        "        return geom_data.DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=config[\"num_workers\"],\n",
        "            pin_memory=config[\"pin_memory\"],\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self) -> geom_data.DataLoader:\n",
        "        \"\"\"\n",
        "        Get the testing dataloader\n",
        "        \"\"\"\n",
        "        return geom_data.DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=config[\"num_workers\"],\n",
        "            pin_memory=config[\"pin_memory\"],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgY_tABw_hsO"
      },
      "source": [
        "## ðŸ¤– The Model\n",
        "\n",
        "> Here is the implementation of our GRN-based Recommendation system. Using LSTM under the hood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2zlPkeaMfHj"
      },
      "outputs": [],
      "source": [
        "class GRN(nn.Module):\n",
        "    def __init__(self, input_channels: int, hidden_channels: int):\n",
        "        \"\"\"\n",
        "        GRN model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels: int\n",
        "            The number of input channels\n",
        "        hidden_channels: int\n",
        "            The number of hidden channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.graph_conv1 = GraphConv(input_channels, hidden_channels)\n",
        "        self.graph_conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lstm = LSTM(hidden_channels, hidden_channels)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_idx: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the GRN\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: Tensor\n",
        "            The input tensor\n",
        "        edge_idx: Tensor\n",
        "            The edge indices\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The output of the GRN\n",
        "        \"\"\"\n",
        "        x = self.graph_conv1(x, edge_idx)\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_conv2(x, edge_idx)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        out, (hidden_state, cell_state) = self.lstm(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Gate definitions\n",
        "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq: Tensor, inital_state: tuple = None) -> tuple:\n",
        "        \"\"\"\n",
        "        Forward pass of the LSTM\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_seq: Tensor\n",
        "            The input sequence\n",
        "        inital_state: tuple\n",
        "            The initial state of the LSTM\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            The output and the state of the LSTM\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = input_seq.size()\n",
        "\n",
        "        if inital_state is None:\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(\n",
        "                input_seq.device\n",
        "            )\n",
        "            cell_state = torch.zeros(batch_size, self.hidden_size).to(input_seq.device)\n",
        "        else:\n",
        "            hidden_state, cell_state = inital_state\n",
        "\n",
        "        hidden_states_over_time = []\n",
        "\n",
        "        for time_step in range(seq_len):\n",
        "            input_at_time_step = input_seq[:, time_step, :]\n",
        "\n",
        "            combined_input = torch.cat((hidden_state, input_at_time_step), dim=1)\n",
        "\n",
        "            input_gate = torch.sigmoid(self.input_gate(combined_input))\n",
        "            forget_gate = torch.sigmoid(self.forget_gate(combined_input))\n",
        "            output_gate = torch.sigmoid(self.output_gate(combined_input))\n",
        "            cell_state_candidate = torch.tanh(self.cell_gate(combined_input))\n",
        "\n",
        "            cell_state = forget_gate * cell_state + input_gate * cell_state_candidate\n",
        "            hidden_state = output_gate * torch.tanh(cell_state)\n",
        "\n",
        "            hidden_states_over_time.append(hidden_state.unsqueeze(0))\n",
        "\n",
        "        hidden_sequence = torch.cat(hidden_states_over_time, dim=0)\n",
        "        hidden_sequence = hidden_sequence.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_sequence, (hidden_state, cell_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HMTGRNModel(pl.LightningModule):\n",
        "    def __init__(self, input_channels: int, hidden_channels: int, num_classes: int):\n",
        "        \"\"\"\n",
        "        HMTGRN model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels: int\n",
        "            The number of input channels\n",
        "        hidden_channels: int\n",
        "            The number of hidden channels\n",
        "        num_classes: int\n",
        "            The number of classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.grn = GRN(input_channels, hidden_channels)\n",
        "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_idx: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the HMTGRN\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: Tensor\n",
        "            The input tensor\n",
        "        edge_idx: Tensor\n",
        "            The edge indices\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The output of the HMTGRN\n",
        "        \"\"\"\n",
        "        x = self.grn(x, edge_idx)\n",
        "        x = self.classifier(x[:, -1, :]) # Get the last output\n",
        "        return x\n",
        "    \n",
        "    def training_step(self, batch: tuple, batch_idx: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Training step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: tuple\n",
        "            The batch\n",
        "        batch_idx: int\n",
        "            The index of the batch\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The loss\n",
        "        \"\"\"\n",
        "        x, edge_idx, y = batch\n",
        "        y_hat = self.forward(x, edge_idx)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch: tuple, batch_idx: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Validation step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: tuple\n",
        "            The batch\n",
        "        batch_idx: int\n",
        "            The index of the batch\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The loss\n",
        "        \"\"\"\n",
        "        x, edge_idx, y = batch\n",
        "        y_hat = self.forward(x, edge_idx)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch: tuple, batch_idx: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        Test step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: tuple\n",
        "            The batch\n",
        "        batch_idx: int\n",
        "            The index of the batch\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            The loss\n",
        "        \"\"\"\n",
        "        x, edge_idx, y = batch\n",
        "        y_hat = self.forward(x, edge_idx)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self) -> optim.Optimizer:\n",
        "        \"\"\"\n",
        "        Configure the optimizer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        optim.Optimizer\n",
        "            The optimizer\n",
        "        \"\"\"\n",
        "        return optim.Adam(self.parameters(), lr=config[\"learning_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgfRTqaJOsoG"
      },
      "source": [
        "## ðŸ‹ï¸â€â™‚ï¸ Training\n",
        "\n",
        "Setting up the training environment and initiating the training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Initialize Data Module and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = TMSCDataModule(\n",
        "    batch_size=config[\"batch_size\"], data_path=config[\"data_path\"]\n",
        ")\n",
        "model = HMTGRNModel(\n",
        "    input_channels=config[\"input_channels\"],\n",
        "    hidden_channels=config[\"hidden_channels\"],\n",
        "    num_classes=config[\"num_classes\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Set Up Training Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\", save_top_k=1, verbose=True, monitor=\"val_loss\", mode=\"min\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Configure Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config[\"max_epochs\"],\n",
        "    accelerator=config[\"accelerator\"],\n",
        "    callbacks=[checkpoint_callback],\n",
        ")\n",
        "\n",
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1JqWgO_802"
      },
      "source": [
        "## ðŸ“ˆ Evaluation\n",
        "\n",
        "Evaluating the model on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDmJZLRr_9JM"
      },
      "outputs": [],
      "source": [
        "model = HMTGRNModel.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
        "trainer.test(model, datamodule=data_module)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
